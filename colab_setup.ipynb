{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Scene Generation on Google Colab\n",
    "\n",
    "This notebook sets up and runs the autoregressive visual scene generation system on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/jtooates/visual-scene-generation.git\n",
    "%cd visual-scene-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed in Colab)\n",
    "!pip install -q tqdm matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Available: {device}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. (Optional) Mount Google Drive for Persistent Storage\n\n**Recommended**: Mount Drive to save your checkpoints permanently. If you skip this, checkpoints will be lost when the session ends!"
  },
  {
   "cell_type": "code",
   "source": "try:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Create directories for saving\n    !mkdir -p /content/drive/MyDrive/visual-scene-generation\n    \n    # Create symlinks so checkpoints save directly to Drive\n    !ln -sf /content/drive/MyDrive/visual-scene-generation/checkpoints /content/visual-scene-generation/checkpoints\n    !ln -sf /content/drive/MyDrive/visual-scene-generation/logs /content/visual-scene-generation/logs\n    \n    print(\"‚úÖ Google Drive mounted! Checkpoints will be saved persistently.\")\n    print(\"üìÅ Checkpoints: /content/drive/MyDrive/visual-scene-generation/checkpoints\")\n    print(\"üìÅ Logs: /content/drive/MyDrive/visual-scene-generation/logs\")\nexcept:\n    print(\"‚ö†Ô∏è Google Drive not mounted. Checkpoints will be temporary!\")\n    print(\"   They will be lost when the session ends.\")\n    !mkdir -p checkpoints logs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Quick Test Run (Small Dataset)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Run a quick test with small dataset to verify everything works\n!python train.py \\\n    --epochs 5 \\\n    --batch_size 16 \\\n    --num_samples 1000 \\\n    --use_vae \\\n    --log_interval 5 \\\n    --lr 0.0001 \\\n    --lambda_kl 0.0001"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Full Training Run"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Full training with recommended settings\n!python train.py \\\n    --epochs 50 \\\n    --batch_size 32 \\\n    --num_samples 10000 \\\n    --use_vae \\\n    --use_amp \\\n    --lr 0.0001 \\\n    --d_model 512 \\\n    --hidden_dim 256 \\\n    --z_dim 128 \\\n    --lambda_consistency 1.0 \\\n    --lambda_spatial 0.1 \\\n    --lambda_kl 0.0001"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Visualize Training Results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Scene Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Interactive Scene Generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport os\nfrom models import AutoregressiveLanguageModel, SceneDecoder, CaptionNetwork\nfrom data_utils import SceneDescriptionDataset\nimport matplotlib.pyplot as plt\n\ndef load_models_from_checkpoint(checkpoint_path=None):\n    \"\"\"\n    Load trained models from checkpoint.\n    If no checkpoint specified, finds the latest one.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Find checkpoint if not specified\n    if checkpoint_path is None:\n        # Check both local and Drive paths\n        checkpoint_dirs = ['checkpoints', '/content/drive/MyDrive/visual-scene-generation/checkpoints']\n        checkpoint_path = None\n        \n        for checkpoint_dir in checkpoint_dirs:\n            if os.path.exists(checkpoint_dir):\n                checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n                if checkpoints:\n                    checkpoint_path = os.path.join(checkpoint_dir, checkpoints[-1])\n                    break\n        \n        if checkpoint_path is None:\n            print(\"‚ùå No checkpoint found! Train the model first.\")\n            return None, None, None, None, device\n    \n    print(f\"Loading checkpoint: {checkpoint_path}\")\n    \n    # Load checkpoint to get vocab size\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # We need to create a dataset to get vocab (or extract from checkpoint if saved)\n    # For now, recreate dataset - it will have same vocab if same seed\n    dataset = SceneDescriptionDataset(num_samples=1000, seed=42)\n    vocab_size = dataset.vocab_size\n    \n    print(f\"Vocabulary size: {vocab_size}\")\n    \n    # Initialize models with same architecture as training\n    ar_model = AutoregressiveLanguageModel(\n        vocab_size=vocab_size,\n        d_model=512,  # Use default or match your training config\n        n_heads=8,\n        n_layers=6\n    ).to(device)\n    \n    scene_decoder = SceneDecoder(\n        embedding_dim=512,\n        hidden_dim=256,\n        use_vae=True,\n        z_dim=128\n    ).to(device)\n    \n    caption_network = CaptionNetwork(\n        vocab_size=vocab_size,\n        embedding_dim=512,\n        hidden_dim=256\n    ).to(device)\n    \n    # Load state dicts\n    try:\n        ar_model.load_state_dict(checkpoint['models']['ar_model'])\n        scene_decoder.load_state_dict(checkpoint['models']['scene_decoder'])\n        caption_network.load_state_dict(checkpoint['models']['caption_network'])\n        \n        # Set to eval mode\n        ar_model.eval()\n        scene_decoder.eval()\n        caption_network.eval()\n        \n        print(f\"‚úÖ Models loaded successfully from epoch {checkpoint['epoch']}\")\n        print(f\"   Training loss: {checkpoint['loss']:.4f}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading models: {e}\")\n        return None, None, None, None, device\n    \n    return ar_model, scene_decoder, caption_network, dataset, device\n\n# Load the models\nar_model, scene_decoder, caption_network, dataset, device = load_models_from_checkpoint()\n\nif ar_model is not None:\n    print(\"\\n‚úÖ Ready for scene generation!\")\nelse:\n    print(\"\\n‚ö†Ô∏è Please train the model first before running generation.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_scene_from_text(text, ar_model, scene_decoder, caption_network, dataset, device):\n    \"\"\"Generate a scene from custom text input\"\"\"\n    if ar_model is None:\n        print(\"‚ùå Models not loaded. Run the previous cell first!\")\n        return\n    \n    ar_model.eval()\n    scene_decoder.eval()\n    caption_network.eval()\n    \n    with torch.no_grad():\n        # Tokenize input text\n        tokens = [dataset.vocab.get(word, dataset.vocab['<UNK>']) for word in text.lower().split()]\n        tokens = [dataset.vocab['<SOS>']] + tokens + [dataset.vocab['<EOS>']]\n        input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n        \n        # Generate embedding\n        ar_outputs = ar_model(input_ids, return_embeddings=True)\n        text_embedding = ar_outputs['embeddings']\n        \n        # Generate scene\n        scene_outputs = scene_decoder(text_embedding)\n        scene = scene_outputs['scene']\n        \n        # Generate caption from scene\n        generated_caption, _ = caption_network.generate_caption(scene)\n        reconstructed_text = dataset.decode_tokens(generated_caption[0])\n        \n        # Visualize\n        scene_np = scene[0].cpu().permute(1, 2, 0).numpy()\n        \n        plt.figure(figsize=(12, 5))\n        plt.subplot(1, 2, 1)\n        plt.imshow(scene_np)\n        plt.title(f\"Original: {text}\", fontsize=12, wrap=True)\n        plt.axis('off')\n        \n        plt.subplot(1, 2, 2)\n        plt.imshow(scene_np)\n        plt.title(f\"Reconstructed: {reconstructed_text}\", fontsize=12, wrap=True)\n        plt.axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        return scene_np, reconstructed_text\n\n# Test with custom text (run after loading models)\nif ar_model is not None:\n    test_texts = [\n        \"a red ball in the center\",\n        \"blue cube on the left\",\n        \"yellow sphere floating\",\n        \"large green triangle\"\n    ]\n    \n    print(\"Generating scenes for test inputs...\\n\")\n    for text in test_texts:\n        print(f\"Input: {text}\")\n        generate_scene_from_text(text, ar_model, scene_decoder, caption_network, dataset, device)\n        print(\"-\" * 50)\nelse:\n    print(\"‚ö†Ô∏è Skip this cell - models not loaded yet\")"
  },
  {
   "cell_type": "code",
   "source": "# Generate scene from your own custom text!\n# Change this to whatever you want:\ncustom_text = \"a tiny red sphere on the right\"\n\nif ar_model is not None:\n    print(f\"Generating scene for: '{custom_text}'\\n\")\n    scene, caption = generate_scene_from_text(\n        custom_text, \n        ar_model, \n        scene_decoder, \n        caption_network, \n        dataset, \n        device\n    )\nelse:\n    print(\"‚ö†Ô∏è Load models first (run cell 8)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume Training from Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Colab\n",
    "\n",
    "1. **Enable GPU**: Go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
    "2. **Prevent Disconnection**: Keep the tab active or use Colab Pro for longer sessions\n",
    "3. **Save Progress**: Regularly save checkpoints to Google Drive\n",
    "4. **Monitor Memory**: Use smaller batch sizes if you encounter OOM errors\n",
    "5. **Use Mixed Precision**: Add `--use_amp` flag for faster training with less memory"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "visual_scene_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}