{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Scene Generation on Google Colab\n",
    "\n",
    "This notebook sets up and runs the autoregressive visual scene generation system on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/jtooates/visual-scene-generation.git\n",
    "%cd visual-scene-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed in Colab)\n",
    "!pip install -q tqdm matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Available: {device}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Test Run (Small Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick test with small dataset to verify everything works\n",
    "!python train.py \\\n",
    "    --epochs 5 \\\n",
    "    --batch_size 16 \\\n",
    "    --num_samples 1000 \\\n",
    "    --use_vae \\\n",
    "    --log_interval 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training with recommended settings\n",
    "!python train.py \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 32 \\\n",
    "    --num_samples 10000 \\\n",
    "    --use_vae \\\n",
    "    --use_amp \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 512 \\\n",
    "    --hidden_dim 256 \\\n",
    "    --z_dim 128 \\\n",
    "    --lambda_consistency 1.0 \\\n",
    "    --lambda_spatial 0.1 \\\n",
    "    --lambda_kl 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Display training curves\n",
    "if os.path.exists('logs/training_curves.png'):\n",
    "    display(Image('logs/training_curves.png'))\n",
    "else:\n",
    "    print(\"Training curves not yet generated\")\n",
    "\n",
    "# Display sample generations\n",
    "if os.path.exists('sample_generations.png'):\n",
    "    display(Image('sample_generations.png'))\n",
    "else:\n",
    "    print(\"Sample generations not yet created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Scene Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import AutoregressiveLanguageModel, SceneDecoder, CaptionNetwork\n",
    "from data_utils import SceneDescriptionDataset\n",
    "from visualization import visualize_scenes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained models\n",
    "def load_trained_models(checkpoint_path='checkpoints/checkpoint_epoch_49.pt'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create dataset for vocabulary\n",
    "    dataset = SceneDescriptionDataset(num_samples=100)\n",
    "    \n",
    "    # Initialize models\n",
    "    ar_model = AutoregressiveLanguageModel(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        d_model=512\n",
    "    ).to(device)\n",
    "    \n",
    "    scene_decoder = SceneDecoder(\n",
    "        embedding_dim=512,\n",
    "        hidden_dim=256,\n",
    "        use_vae=True\n",
    "    ).to(device)\n",
    "    \n",
    "    caption_network = CaptionNetwork(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        embedding_dim=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        ar_model.load_state_dict(checkpoint['models']['ar_model'])\n",
    "        scene_decoder.load_state_dict(checkpoint['models']['scene_decoder'])\n",
    "        caption_network.load_state_dict(checkpoint['models']['caption_network'])\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found, using random initialization\")\n",
    "    \n",
    "    return ar_model, scene_decoder, caption_network, dataset, device\n",
    "\n",
    "# Load models\n",
    "ar_model, scene_decoder, caption_network, dataset, device = load_trained_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene_from_text(text, ar_model, scene_decoder, caption_network, dataset, device):\n",
    "    \"\"\"Generate a scene from custom text input\"\"\"\n",
    "    ar_model.eval()\n",
    "    scene_decoder.eval()\n",
    "    caption_network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input text\n",
    "        tokens = [dataset.vocab.get(word, dataset.vocab['<UNK>']) for word in text.lower().split()]\n",
    "        tokens = [dataset.vocab['<SOS>']] + tokens + [dataset.vocab['<EOS>']]\n",
    "        input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate embedding\n",
    "        ar_outputs = ar_model(input_ids, return_embeddings=True)\n",
    "        text_embedding = ar_outputs['embeddings']\n",
    "        \n",
    "        # Generate scene\n",
    "        scene_outputs = scene_decoder(text_embedding)\n",
    "        scene = scene_outputs['scene']\n",
    "        \n",
    "        # Generate caption from scene\n",
    "        generated_caption, _ = caption_network.generate_caption(scene)\n",
    "        reconstructed_text = dataset.decode_tokens(generated_caption[0])\n",
    "        \n",
    "        # Visualize\n",
    "        scene_np = scene[0].cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(scene_np)\n",
    "        plt.title(f\"Original: {text}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(scene_np)\n",
    "        plt.title(f\"Reconstructed: {reconstructed_text}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test with custom text\n",
    "test_texts = [\n",
    "    \"a red ball in the center\",\n",
    "    \"blue cube on the left\",\n",
    "    \"yellow sphere floating\",\n",
    "    \"large green triangle\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    generate_scene_from_text(text, ar_model, scene_decoder, caption_network, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy checkpoints to Drive\n",
    "!cp -r checkpoints /content/drive/MyDrive/visual-scene-generation-checkpoints\n",
    "!cp -r logs /content/drive/MyDrive/visual-scene-generation-logs\n",
    "print(\"Models saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume Training from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from a saved checkpoint\n",
    "!python train.py \\\n",
    "    --resume checkpoints/checkpoint_epoch_49.pt \\\n",
    "    --epochs 100 \\\n",
    "    --batch_size 32 \\\n",
    "    --num_samples 10000 \\\n",
    "    --use_vae \\\n",
    "    --use_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Colab\n",
    "\n",
    "1. **Enable GPU**: Go to Runtime → Change runtime type → Hardware accelerator → GPU\n",
    "2. **Prevent Disconnection**: Keep the tab active or use Colab Pro for longer sessions\n",
    "3. **Save Progress**: Regularly save checkpoints to Google Drive\n",
    "4. **Monitor Memory**: Use smaller batch sizes if you encounter OOM errors\n",
    "5. **Use Mixed Precision**: Add `--use_amp` flag for faster training with less memory"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "visual_scene_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}