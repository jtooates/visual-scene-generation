{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Scene Generation on Google Colab\n",
    "\n",
    "This notebook sets up and runs the autoregressive visual scene generation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (Optional) Reset Environment\n",
    "\n",
    "Run this if you're re-running the notebook or encounter nested directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing setup\n",
    "import os\n",
    "%cd /content\n",
    "!rm -rf visual-scene-generation\n",
    "print(\"‚úÖ Environment reset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository to specific location\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/content/visual-scene-generation'):\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone https://github.com/jtooates/visual-scene-generation.git /content/visual-scene-generation\n",
    "else:\n",
    "    print(\"Repository exists, pulling latest...\")\n",
    "    !cd /content/visual-scene-generation && git pull\n",
    "\n",
    "%cd /content/visual-scene-generation\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
    "!ls -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tqdm matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mount Google Drive (CRITICAL for saving checkpoints!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create directories on Google Drive\n",
    "    !mkdir -p /content/drive/MyDrive/visual-scene-generation/checkpoints\n",
    "    !mkdir -p /content/drive/MyDrive/visual-scene-generation/logs\n",
    "    \n",
    "    # Remove any local directories\n",
    "    !rm -rf /content/visual-scene-generation/checkpoints\n",
    "    !rm -rf /content/visual-scene-generation/logs\n",
    "    \n",
    "    # Create symlinks: local -> Drive\n",
    "    !ln -s /content/drive/MyDrive/visual-scene-generation/checkpoints /content/visual-scene-generation/checkpoints\n",
    "    !ln -s /content/drive/MyDrive/visual-scene-generation/logs /content/visual-scene-generation/logs\n",
    "    \n",
    "    print(\"‚úÖ Google Drive mounted and symlinks created!\")\n",
    "    print(\"üìÅ Checkpoints ‚Üí /content/drive/MyDrive/visual-scene-generation/checkpoints\")\n",
    "    print(\"üìÅ Logs ‚Üí /content/drive/MyDrive/visual-scene-generation/logs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Drive mount failed: {e}\")\n",
    "    print(\"Creating local directories (will be lost when session ends)\")\n",
    "    !mkdir -p checkpoints logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\\n\")\n",
    "\n",
    "# Check checkpoints\n",
    "if os.path.islink('checkpoints'):\n",
    "    target = os.readlink('checkpoints')\n",
    "    print(f\"‚úÖ checkpoints/ ‚Üí {target}\")\n",
    "elif os.path.exists('checkpoints'):\n",
    "    print(f\"‚ö†Ô∏è checkpoints/ is a regular directory (not linked to Drive!)\")\n",
    "else:\n",
    "    print(\"‚ùå checkpoints/ doesn't exist (will be created by training script)\")\n",
    "\n",
    "# Check logs\n",
    "if os.path.islink('logs'):\n",
    "    target = os.readlink('logs')\n",
    "    print(f\"‚úÖ logs/ ‚Üí {target}\")\n",
    "elif os.path.exists('logs'):\n",
    "    print(f\"‚ö†Ô∏è logs/ is a regular directory (not linked to Drive!)\")\n",
    "else:\n",
    "    print(\"‚ùå logs/ doesn't exist (will be created by training script)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"If symlinks are shown, checkpoints WILL persist to Drive!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Training (5 epochs, ~5-10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python train.py \\\n    --epochs 5 \\\n    --batch_size 16 \\\n    --num_samples 1000 \\\n    --use_vae \\\n    --lr 0.0001 \\\n    --lambda_kl 0.001 \\\n    --log_interval 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Training (50 epochs, ~30-60 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!python train.py \\\n    --epochs 50 \\\n    --batch_size 16 \\\n    --num_samples 10000 \\\n    --use_vae \\\n    --lr 0.00005 \\\n    --d_model 512 \\\n    --hidden_dim 256 \\\n    --z_dim 128 \\\n    --lambda_consistency 1.0 \\\n    --lambda_spatial 0.1 \\\n    --lambda_kl 0.001"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "if os.path.exists('logs/training_curves.png'):\n",
    "    print(\"Training Curves:\")\n",
    "    display(Image('logs/training_curves.png'))\n",
    "\n",
    "if os.path.exists('sample_generations.png'):\n",
    "    print(\"\\nSample Generations:\")\n",
    "    display(Image('sample_generations.png'))\n",
    "\n",
    "print(\"\\nüìÅ Checkpoints:\")\n",
    "!ls -lh checkpoints/ 2>/dev/null || echo \"No checkpoints yet\"\n",
    "\n",
    "print(\"\\nüìÅ Logs:\")\n",
    "!ls -lh logs/ 2>/dev/null || echo \"No logs yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Models and Generate Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from models import AutoregressiveLanguageModel, SceneDecoder, CaptionNetwork\n",
    "from data_utils import SceneDescriptionDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_checkpoint():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Find latest checkpoint\n",
    "    checkpoint_dirs = ['checkpoints', '/content/drive/MyDrive/visual-scene-generation/checkpoints']\n",
    "    checkpoint_path = None\n",
    "    \n",
    "    for cp_dir in checkpoint_dirs:\n",
    "        if os.path.exists(cp_dir):\n",
    "            files = sorted([f for f in os.listdir(cp_dir) if f.endswith('.pt')])\n",
    "            if files:\n",
    "                checkpoint_path = os.path.join(cp_dir, files[-1])\n",
    "                break\n",
    "    \n",
    "    if not checkpoint_path:\n",
    "        print(\"‚ùå No checkpoint found! Train first.\")\n",
    "        return None, None, None, None, device\n",
    "    \n",
    "    print(f\"Loading: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Recreate dataset for vocab\n",
    "    dataset = SceneDescriptionDataset(num_samples=1000, seed=42)\n",
    "    \n",
    "    # Initialize models\n",
    "    ar_model = AutoregressiveLanguageModel(\n",
    "        vocab_size=dataset.vocab_size, d_model=512\n",
    "    ).to(device)\n",
    "    \n",
    "    scene_decoder = SceneDecoder(\n",
    "        embedding_dim=512, hidden_dim=256, use_vae=True\n",
    "    ).to(device)\n",
    "    \n",
    "    caption_network = CaptionNetwork(\n",
    "        vocab_size=dataset.vocab_size, embedding_dim=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    ar_model.load_state_dict(checkpoint['models']['ar_model'])\n",
    "    scene_decoder.load_state_dict(checkpoint['models']['scene_decoder'])\n",
    "    caption_network.load_state_dict(checkpoint['models']['caption_network'])\n",
    "    \n",
    "    ar_model.eval()\n",
    "    scene_decoder.eval()\n",
    "    caption_network.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded from epoch {checkpoint['epoch']}\")\n",
    "    return ar_model, scene_decoder, caption_network, dataset, device\n",
    "\n",
    "ar_model, scene_decoder, caption_network, dataset, device = load_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Custom Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene(text):\n",
    "    if ar_model is None:\n",
    "        print(\"‚ùå Load models first!\")\n",
    "        return\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize\n",
    "        tokens = [dataset.vocab.get(w, dataset.vocab['<UNK>']) for w in text.lower().split()]\n",
    "        tokens = [dataset.vocab['<SOS>']] + tokens + [dataset.vocab['<EOS>']]\n",
    "        input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        ar_out = ar_model(input_ids, return_embeddings=True)\n",
    "        scene_out = scene_decoder(ar_out['embeddings'])\n",
    "        caption_tokens, _ = caption_network.generate_caption(scene_out['scene'])\n",
    "        reconstructed = dataset.decode_tokens(caption_tokens[0])\n",
    "        \n",
    "        # Visualize\n",
    "        scene_np = scene_out['scene'][0].cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(scene_np)\n",
    "        plt.title(f\"Input: {text}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(scene_np)\n",
    "        plt.title(f\"Reconstructed: {reconstructed}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Try it!\n",
    "if ar_model is not None:\n",
    "    for text in [\"red ball in center\", \"blue cube on left\", \"yellow sphere floating\"]:\n",
    "        generate_scene(text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "visual_scene_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}