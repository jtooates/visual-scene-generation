{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Scene Generation on Google Colab\n",
    "\n",
    "This notebook sets up and runs the autoregressive visual scene generation system on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone the Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/jtooates/visual-scene-generation.git\n",
    "%cd visual-scene-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed in Colab)\n",
    "!pip install -q tqdm matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Available: {device}\")\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. (Optional) Mount Google Drive for Persistent Storage\n\n**Recommended**: Mount Drive to save your checkpoints permanently. If you skip this, checkpoints will be lost when the session ends!"
  },
  {
   "cell_type": "code",
   "source": "try:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Create directories for saving\n    !mkdir -p /content/drive/MyDrive/visual-scene-generation\n    \n    # Create symlinks so checkpoints save directly to Drive\n    !ln -sf /content/drive/MyDrive/visual-scene-generation/checkpoints /content/visual-scene-generation/checkpoints\n    !ln -sf /content/drive/MyDrive/visual-scene-generation/logs /content/visual-scene-generation/logs\n    \n    print(\"‚úÖ Google Drive mounted! Checkpoints will be saved persistently.\")\n    print(\"üìÅ Checkpoints: /content/drive/MyDrive/visual-scene-generation/checkpoints\")\n    print(\"üìÅ Logs: /content/drive/MyDrive/visual-scene-generation/logs\")\nexcept:\n    print(\"‚ö†Ô∏è Google Drive not mounted. Checkpoints will be temporary!\")\n    print(\"   They will be lost when the session ends.\")\n    !mkdir -p checkpoints logs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Quick Test Run (Small Dataset)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Run a quick test with small dataset to verify everything works\n!python train.py \\\n    --epochs 5 \\\n    --batch_size 16 \\\n    --num_samples 1000 \\\n    --use_vae \\\n    --log_interval 5 \\\n    --lr 0.0001 \\\n    --lambda_kl 0.0001"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Full Training Run"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Full training with recommended settings\n!python train.py \\\n    --epochs 50 \\\n    --batch_size 32 \\\n    --num_samples 10000 \\\n    --use_vae \\\n    --use_amp \\\n    --lr 0.0001 \\\n    --d_model 512 \\\n    --hidden_dim 256 \\\n    --z_dim 128 \\\n    --lambda_consistency 1.0 \\\n    --lambda_spatial 0.1 \\\n    --lambda_kl 0.0001"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Visualize Training Results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Scene Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Interactive Scene Generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene_from_text(text, ar_model, scene_decoder, caption_network, dataset, device):\n",
    "    \"\"\"Generate a scene from custom text input\"\"\"\n",
    "    ar_model.eval()\n",
    "    scene_decoder.eval()\n",
    "    caption_network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input text\n",
    "        tokens = [dataset.vocab.get(word, dataset.vocab['<UNK>']) for word in text.lower().split()]\n",
    "        tokens = [dataset.vocab['<SOS>']] + tokens + [dataset.vocab['<EOS>']]\n",
    "        input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Generate embedding\n",
    "        ar_outputs = ar_model(input_ids, return_embeddings=True)\n",
    "        text_embedding = ar_outputs['embeddings']\n",
    "        \n",
    "        # Generate scene\n",
    "        scene_outputs = scene_decoder(text_embedding)\n",
    "        scene = scene_outputs['scene']\n",
    "        \n",
    "        # Generate caption from scene\n",
    "        generated_caption, _ = caption_network.generate_caption(scene)\n",
    "        reconstructed_text = dataset.decode_tokens(generated_caption[0])\n",
    "        \n",
    "        # Visualize\n",
    "        scene_np = scene[0].cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(scene_np)\n",
    "        plt.title(f\"Original: {text}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(scene_np)\n",
    "        plt.title(f\"Reconstructed: {reconstructed_text}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test with custom text\n",
    "test_texts = [\n",
    "    \"a red ball in the center\",\n",
    "    \"blue cube on the left\",\n",
    "    \"yellow sphere floating\",\n",
    "    \"large green triangle\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    generate_scene_from_text(text, ar_model, scene_decoder, caption_network, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resume Training from Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Colab\n",
    "\n",
    "1. **Enable GPU**: Go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
    "2. **Prevent Disconnection**: Keep the tab active or use Colab Pro for longer sessions\n",
    "3. **Save Progress**: Regularly save checkpoints to Google Drive\n",
    "4. **Monitor Memory**: Use smaller batch sizes if you encounter OOM errors\n",
    "5. **Use Mixed Precision**: Add `--use_amp` flag for faster training with less memory"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "visual_scene_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}